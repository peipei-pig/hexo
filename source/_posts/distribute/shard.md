
---
title: pytorch Shard
categories:
  - åˆ†å¸ƒå¼åŸºç¡€
tags:
  - shard
description: pytorchä¸­Shardå®ç°
---

<!-- more -->

---

## 1. _split_tensoråˆ†æ

### 1.1 ä»£ç å®ç°æµç¨‹å›¾ï¼ˆMermaidï¼‰

```mermaid
flowchart TD
  A["è¾“å…¥ï¼štensor, num_chunks, with_padding, contiguous"] --> B{"dim â‰¤ tensor.ndim?"}
  B -- å¦ --> E["AssertionError æŠ›å‡º"]
  B -- æ˜¯ --> C["è°ƒç”¨ torch.chunk æ²¿ dim åˆ†å—"]
  C --> D["tensor_list, è®¡ç®— num_empty_tensors = num_chunks - len(tensor_list)"]
  D --> F{"æ— éœ€ padding æˆ– å‡åŒ€å¯åˆ†?"}
  F -- æ˜¯ --> G["(å¯é€‰) å¯¹æ¯å—è°ƒç”¨ .contiguous()"]
  G --> H["è°ƒç”¨ fill_empty_tensor_to_shards è¡¥ç©º shard"]
  H --> I["è¿”å› shards åˆ—è¡¨ å’Œ ç©º pad_sizes []"]
  F -- å¦ --> J["è®¡ç®— full_chunk_size = ceil(dim_size / num_chunks)"]
  J --> K["æ”¶é›†åŸå§‹ chunk_sizes"]
  K --> L["pad_sizes = full_chunk_size - chunk_size"]
  L --> M["è°ƒç”¨ fill_empty_tensor_to_shards è¡¥ç©º shard"]
  M --> N["å¯¹æ¯ä¸ª shardï¼šè‹¥ pad_size > 0ï¼Œåˆ™ pad_tensor(shard, dim, pad_size)"]
  N --> O["(å¯é€‰) shard.contiguous()"]
  O --> P["æ”¶é›† shard_list å’Œ pad_sizes"]
  P --> Q["è¿”å› shard_list å’Œ pad_sizes"]
```

---

### 1.2 å…³é”®ç‚¹è¯¦è§£

#### ğŸ§  ä¸ºä»€ä¹ˆè¦ Paddingï¼Ÿ

ç”¨äºä¿è¯åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ï¼ˆæ¯”å¦‚ `scatter`ã€`all_gather` ç­‰ collective æ“ä½œï¼‰æ¯ä¸ª rank çš„ shard å¤§å°ä¸€è‡´ï¼Œé¿å…å› ä¸ºå°ºå¯¸ä¸å¯¹é½å¯¼è‡´é€šä¿¡å¤±è´¥ã€‚åªæœ‰ `tensor.size(dim) % num_chunks â‰  0` ä¸” `with_padding=True` æ—¶ï¼Œæ‰ä¼šè¿›è¡Œ paddingã€‚

#### ğŸ§© `fill_empty_tensor_to_shards`

`torch.chunk` åœ¨å°ºå¯¸è¾ƒå°æˆ– `num_chunks` æ›´å¤§æ—¶ä¸ä¼šè¾“å‡ºç©º tensorã€‚è¯¥å‡½æ•°ç”¨äºè¡¥å…¨ï¼šåœ¨ `tensor_list` å°‘äº `num_chunks` æ—¶ï¼Œè¡¥å……å½¢çŠ¶åˆæ³•ä½† dim ä¸Šä¸º 0 çš„ç©º tensorï¼Œä½¿ shard æ•°ç›®ä¸€è‡´ï¼Œä¾¿äºåç»­ç»Ÿä¸€å¤„ç†ã€‚

#### ğŸ§¼ `pad_tensor`

è‹¥å½“å‰ shard å°äº `full_chunk_size`ï¼Œåˆ™åœ¨æŒ‡å®šç»´åº¦æœ«å°¾è¡¥é›¶ï¼Œç¡®ä¿æ‰€æœ‰ shard çš„å½¢çŠ¶ä¸€è‡´ã€‚

#### ğŸ§± `contiguous`

ä¸ºæå‡å†…å­˜è¿è´¯æ€§å’Œé€šä¿¡æ•ˆç‡ï¼Œå¯è°ƒç”¨ `.contiguous()` é‡æ’å†…å­˜å¸ƒå±€ã€‚

---

### 1.3 å®é™…è°ƒç”¨ç¤ºä¾‹ï¼ˆéœ€ Paddingï¼‰

ä»¥ä¸‹ä¸ºæ— æ³•å‡åŒ€åˆ†ç‰‡ï¼Œå›  `num_chunks=4` è€Œè§¦å‘ pad çš„åœºæ™¯ï¼š

```python
import torch
from torch.distributed.tensor.placement_types import Shard

# æ„é€ å¼ é‡
tensor = torch.arange(1, 13).reshape(2, 6)  # shape [2, 6]

# åœ¨ dim=1 ä¸Šæ‹†ä¸º 4 ä»½ï¼Œä¸æ•´é™¤å°†è§¦å‘ padding
sharder = Shard(dim=1)
shards, pad_sizes = sharder._split_tensor(tensor, num_chunks=4, with_padding=True)

print("Pad sizes:", pad_sizes)
for i, (sh, pad) in enumerate(zip(shards, pad_sizes)):
    print(f"Shard {i} shape: {tuple(sh.shape)}, pad: {pad}")
    print(sh)
```

#### âœ… é¢„æœŸç»“æœ

* `tensor.size(1)=6`, `num_chunks=4` â‡’ `full_chunk_size = ceil(6/4) = 2`
* `torch.chunk` ä¼šå‡º 4 å—ï¼Œä½†æœ€åä¸€ä¸¤å—å¯èƒ½ä¸º empty
* pad\_sizes å¯èƒ½ä¸º `[0, 0, 0, 2]`
* æœ€ç»ˆæ¯å—å¤§å°éƒ½æ˜¯ `[2]` (dim=1)ï¼Œpadding è¡¥é½

```
Pad sizes: [0, 0, 0, 2]
Shard 0 shape: (2, 2), pad: 0
tensor([[1, 2],
        [7, 8]])
Shard 1 shape: (2, 2), pad: 0
tensor([[ 3,  4],
        [ 9, 10]])
Shard 2 shape: (2, 2), pad: 0
tensor([[ 5,  6],
        [11, 12]])
Shard 3 shape: (2, 2), pad: 2
tensor([[0, 0],
        [0, 0]])
```

---

### 1.4 æ€»ç»“

* `_split_tensor` çš„ä½œç”¨æ˜¯**å°†ä¸€ä¸ª Tensor æ²¿æŒ‡å®šç»´åº¦åˆ‡åˆ†ä¸ºå›ºå®šä»½æ•°**ï¼Œå¹¶åœ¨ **ä¸èƒ½æ•´é™¤æ—¶è‡ªåŠ¨è¡¥é½**ã€‚
* å®ƒä¿éšœäº†å„ shard åœ¨é€šä¿¡é˜¶æ®µå°ºå¯¸ä¸€è‡´ï¼Œ**é€‚ç”¨äºåˆ†å¸ƒå¼å¼ é‡å¹¶è¡Œåœºæ™¯**ã€‚
* å®é™…ä»£ç é€šè¿‡ `torch.chunk`ã€`fill_empty_tensor_to_shards`ã€`pad_tensor` ç­‰æ‰‹æ®µï¼Œè½»æ¾å®ç°è¿™ä¸€ç›®æ ‡ã€‚

---



