
---
title: pytorch devicemesh
date: 2025-06-14 10:00:00
categories:
  - åˆ†å¸ƒå¼åŸºç¡€
tags:
  - devicemesh
description: pytorchä¸­devicemeshå®ç°
---

<!-- more -->

---

## ä¸€ã€ä¸ºä½•ä½¿ç”¨ DeviceMeshï¼Ÿ

åœ¨æ··åˆå¹¶è¡Œï¼ˆDP/TP/PP/HSDP/â€¦ï¼‰ä¸­ï¼Œéœ€è¦ç®¡ç†å¤šä¸ªå­é€šä¿¡ç»„ï¼ˆProcessGroupï¼‰ï¼Œå¯¹åº”å¤æ‚çš„è®¾å¤‡æ‹“æ‰‘ç»“æ„ã€‚`DeviceMesh` æä¾›äº†ï¼š

* ç†è®ºä¸Šæ— ç¼æ”¯æŒä»»æ„ç»´åº¦çš„å¤šç»´æ‹“æ‰‘ï¼›
* è‡ªåŠ¨æ‹†åˆ†è¿›ç¨‹ç»„(`new_group`/`split_group`)ï¼›
* çµæ´»åˆ‡ç‰‡å­ Meshï¼›
* ç»å†è®¾è®¡å‘¨å…¨çš„é«˜æ•ˆåˆå§‹åŒ–æ–¹æ¡ˆ ([docs.pytorch.org][1], [pytorch.org][2])ã€‚

---

## äºŒã€åˆå§‹åŒ–æµç¨‹

### `init_device_mesh(...)` çš„ä½œç”¨

ä¸€ä¸ªä¸€è¡Œæå®šçš„æ–¹æ³•ï¼Œå®ƒä¼šï¼š

1. åˆå§‹åŒ–å…¨å±€ `init_process_group(...)`ï¼ˆè‹¥æœªåˆå§‹åŒ–ï¼‰ï¼›
2. æ ¹æ® `mesh_shape` è‡ªåŠ¨æ„é€  CPU ä¸Šçš„ `torch.arange(...).view(...)`ï¼›
3. åˆ›å»º `DeviceMesh(...)`ã€‚å†…éƒ¨å®Œæˆå­ç»„æ‹†åˆ†åŸç†ï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰ã€‚

---

### `DeviceMesh.__init__()` + `_init_process_groups()`

* **å­˜å‚¨**ï¼š`device_type`ã€`mesh`ã€`mesh_dim_names`ï¼›
* **é€šä¿¡ç»„æ‹†åˆ†**ï¼šéå†æ¯ä¸ªç»´åº¦ `dim`ï¼š

  * ä½¿ç”¨ `mesh.swapdims(-1, dim).reshape(-1, size(dim))` åˆ—å‡ºè¯¥ç»´æ‰€æœ‰å­ç»„ rankï¼›
  * è‹¥ NCCL å·²ç»‘å®š GPUï¼Œå³å¯ç”¨ `split_group` ä¸€æ¬¡æ‹†å‡ºå…¨éƒ¨å­ç»„ï¼›
  * å¦åˆ™ä½¿ç”¨ `new_group()` åˆ† group æ‹†ï¼›
  * å¹¶å°†å½“å‰ rank å±äºçš„é‚£ç»„ä¿¡æ¯æ”¾å…¥ `self._dim_group_infos[dim]`ï¼›
* **ç»“æœ**ï¼šæ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªåŒ…å«å½“å‰ rank çš„ `ProcessGroup` ä¿¡æ¯åˆ—è¡¨ã€‚

```python
#pp
mesh = torch.tensor([
  [0, 1],  # pp=0
  [2, 3],  # pp=1
  [4, 5],  # pp=2
  [6, 7]   # pp=3
])

mesh.swapdims(-1, 0)

tensor([[0,2,4,6],
        [1,3,5,7]])

pg_ranks_by_dim = tmp.reshape(-1, mesh.size(0))

[
  [0,2,4,6],  # å¯¹åº” tp è¡Œ 0 å„ pp æ®µ
  [1,3,5,7]   # å¯¹åº” tp è¡Œ 1 å„ pp æ®µ
]

#tp

tmp = mesh.swapdims(-1, 1)  # ç­‰äº transpose(1,1)ï¼Œæœ¬èº«æ— å˜åŒ–
pg_ranks_by_dim = tmp.reshape(-1, mesh.size(1))

[
  [0,1],  # pp=0
  [2,3],
  [4,5],
  [6,7]
]
```

---

## ä¸‰ã€æ ¸å¿ƒæ¥å£ä¸å†…éƒ¨å®ç°è§£æ

### 1. å±æ€§ä¸æ–¹æ³•

```python
mesh.shape  # tuple(self.mesh.shape)
mesh.ndim   # int(self.mesh.ndim)
mesh.size(dim=None)  # æ€»å…ƒç´ æ•° or self.mesh.size(dim)
```

ç”¨äºè·å– mesh å…ƒç»“æ„å’Œè§„æ¨¡ï¼Œé€‚ç”¨äºåˆ¤æ–­ç»´åº¦æ•°é‡ã€å¾ªç¯è¿­ä»£ã€å¹¶è¡Œç­–ç•¥é…ç½®ç­‰åœºæ™¯ã€‚

---

### 2. Rank ä¸åæ ‡

* `get_rank()`ï¼šç­‰ä»·äº `torch.distributed.get_rank()`ï¼Œè¿”å›å…¨å±€ rankï¼›
* `get_local_rank(mesh_dim)`ï¼šå†…éƒ¨è°ƒç”¨ `get_rank(self.get_group(mesh_dim))` â†’ å½“å‰ç»´åº¦çš„å°ç»„å†…ç¼–å·ï¼›
* `get_coordinate()`ï¼šè¿”å› `self._coordinate_on_dim`ï¼Œå…¶åœ¨åˆå§‹åŒ–ä¸­é€šè¿‡ `(self.mesh==global_rank).nonzero()` è·å¾—ã€‚

ç¤ºä¾‹ï¼š`mesh_shape=(4,2)`ï¼Œrank=5 â†’ local\_pp=2ã€local\_tp=1ï¼Œcoordinate `[2,1]`ã€‚

---

### 3. é€šä¿¡ç»„è·å–

* `get_group(mesh_dim)`ï¼š

  * è‹¥ 1D ä¸”ä¸ä¼ å‚ï¼Œç›´æ¥è¿”å›å”¯ä¸€å­è¿›ç¨‹ç»„ï¼›
  * å¤šç»´åˆ™æ ¹æ® `mesh_dim`ï¼ˆç´¢å¼•æˆ–åå­—ï¼‰æ£€ç´¢ `self._dim_group_infos[dim]`ï¼Œç”¨ `_find_pg_by_ranks_and_tag()` è·å–å¯¹åº” `ProcessGroup`ã€‚
* `get_all_groups()`ï¼šè¿”å›æ‰€æœ‰ç»´åº¦çš„ group åˆ—è¡¨ï¼›
* `__getitem__(dims)`ï¼šåˆ‡ç‰‡æ¥å£è°ƒç”¨ `_mesh_resources._get_slice_mesh_dims(...)`ï¼Œåˆ›å»ºæ–°çš„å­ meshï¼Œä¿ç•™åº•å±‚ communicatorï¼Œä½†ç»´åº¦é™ã€‚

  * æ”¯æŒå•ç»´æˆ–å¤šç»´åˆ‡ç‰‡ï¼Œä¸”è¿”å›çš„ submesh é¡ºåºæŒ‰ä¼ å…¥é¡ºåºæ’åˆ— ([discuss.ray.io][3], [gemfury.com][4], [pytorch.org][2])ã€‚

---

### 4. `from_group(...)` æ–¹æ³•

* å¯æ¥å—å• group æˆ– group åˆ—è¡¨ï¼›
* åˆ›å»ºæ–°çš„ `DeviceMesh` æ—¶ä¸ä¼šè°ƒç”¨ backend åˆå§‹åŒ–ï¼›
* ä¼šå¤ç”¨ç°æœ‰ `ProcessGroup`ï¼Œå¹¶å¡«å…… `_dim_group_infos`ï¼Œå› æ­¤ `get_group(...)` å°†ç›´æ¥è¿”å›ä¼ å…¥çš„å®ä¾‹ï¼Œé¿å…é‡å¤åˆ›å»º groupã€‚

---

## å››ã€å®Œæ•´å•æœº 8 å¡ Demoï¼štp=2, pp=4

ä¸‹é¢æ¼”ç¤ºå¦‚ä½•è°ƒç”¨æ‰€æœ‰æ¥å£å¹¶è¾“å‡ºç»“æœã€‚æ³¨æ„ï¼šéœ€åœ¨ `torchrun --nproc_per_node=8` ä¸‹è¿è¡Œã€‚

```python
import os, torch, torch.distributed as dist
from torch.distributed.device_mesh import init_device_mesh

def run_device_mesh_demo():
    dist.init_process_group("nccl")
    # â¬‡ï¸ åˆå§‹åŒ– 2-ç»´ meshï¼špp=4, tp=2
    mesh = init_device_mesh("cuda", mesh_shape=(4, 2), mesh_dim_names=("pp", "tp"))
    
    # âœ… rank å’Œåæ ‡
    gr = mesh.get_rank()            # å…¨å±€ rank
    coord = mesh.get_coordinate()   # [pp_idx, tp_idx]
    local_pp = mesh.get_local_rank("pp")
    local_tp = mesh.get_local_rank("tp")
    
    # â¬‡ï¸ mesh åŸºæœ¬ç»“æ„
    total = mesh.size()
    pp_size, tp_size = mesh.size("pp"), mesh.size("tp")
    ndim = mesh.ndim
    shape = mesh.shape
    
    # â¬‡ï¸ è·å–é€šä¿¡ç»„
    pp_group = mesh.get_group("pp")
    tp_group = mesh.get_group("tp")
    all_groups = mesh.get_all_groups()
    
    # â¬‡ï¸ åˆ‡ç‰‡å‡ºå­ mesh
    tp_mesh = mesh["tp"]
    pp_mesh = mesh["pp"]
    
    # â¬‡ï¸ è¾“å‡ºç»“æœ
    print(f"rank={gr}, coord={coord}, local_pp={local_pp}, local_tp={local_tp}")
    print(f"ndim={ndim}, shape={shape}, total={total}, pp={pp_size}, tp={tp_size}")
    print("pp_group ranks:", dist.get_process_group_ranks(pp_group))
    print("tp_group ranks:", dist.get_process_group_ranks(tp_group))
    print("all_groups sizes:", [len(dist.get_process_group_ranks(g)) for g in all_groups])
    print("tp_mesh ndim, shape:", tp_mesh.ndim, tp_mesh.shape)
    print("pp_mesh ndim, shape:", pp_mesh.ndim, pp_mesh.shape)

if __name__ == "__main__":
    run_device_mesh_demo()
```

### ğŸ’¬ é¢„æœŸè¾“å‡ºï¼ˆä¾‹å¦‚ rank = 5ï¼‰ï¼š

rank=5, coord=\[2,1], local\_pp=2, local\_tp=1
ndim=2, shape=(4,2), total=8, pp=4, tp=2
pp\_group ranks: \[4,5,6,7]
tp\_group ranks: \[5,7]
all\_groups sizes: \[4,2]
tp\_mesh ndim, shape: 1 (2,)
pp\_mesh ndim, shape: 1 (4,)


è¯´æ˜ï¼š
- rank=5 ä½äº pipeline æ®µ 2ï¼Œtp å†…ç¼–å· 1ï¼›
- `pp_group` åŒ…å«ä¸å…¶åŒ segment çš„ 4 å¼ å¡ï¼›
- `tp_group` åŒ…å«åŒ segment tp ç»´åº¦çš„ä¸¤å¼ å¡ï¼›
- åˆ‡ç‰‡å `tp_mesh`ã€`pp_mesh` æˆä¸º 1 ç»´ç»“æ„ï¼Œç”¨äºåç»­ parallelizationã€‚

---

## ğŸ‘ æ€»ç»“

- `DeviceMesh` æ„å»ºè‡ªèº«é€šè¿‡ `init_device_mesh()` å®Œæˆåˆå§‹åŒ–ä¸å­ç»„æ‹†åˆ†ï¼›
- æ¥å£å†…éƒ¨å®ç°é€»è¾‘ä¸ Group ç®¡ç†æœºåˆ¶æ¸…æ™°ã€é«˜æ•ˆï¼›
- `__getitem__`ä¸ºå¤šç»´å¹¶è¡Œä¸‹å­ Mesh åˆ‡ç‰‡å…³é”®å·¥å…·ï¼Œå¯¹é›†æˆ parallel APIs è‡³å…³é‡è¦ï¼›
- é€šè¿‡è¯¥æœºåˆ¶ï¼Œå¯ä»¥ç®€å•åœ°ç»„ç»‡å¤æ‚çš„ hybrid-parallel pipelinesï¼ŒåŒæ—¶å……åˆ†å¤ç”¨ communicator èµ„æºå¹¶ç®€åŒ–å¼€å‘æµç¨‹ã€‚

[1]: https://docs.pytorch.org/tutorials/recipes/distributed_device_mesh.html?utm_source=chatgpt.com "Getting Started with DeviceMesh - PyTorch documentation"
[2]: https://pytorch.org/docs/stable/distributed.html?utm_source=chatgpt.com "Distributed communication package - torch.distributed"
[3]: https://discuss.ray.io/t/init-device-mesh-in-pytorch-distributed/22371?utm_source=chatgpt.com "Init device mesh in pytorch distributed - Ray Train"
[4]: https://gemfury.com/turingmotors/python%3Atorch/torch-2.1.2-cp311-cp311-linux_aarch64.whl/content/distributed/_tensor/device_mesh.py?utm_source=chatgpt.com "distributed/_tensor/device_mesh.py Â· turingmotors/torch - Gemfury"

